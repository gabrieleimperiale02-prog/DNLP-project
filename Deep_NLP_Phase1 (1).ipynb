{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep NLP Project - Phase 1\n",
        "\n",
        "## Prompt-Based Abstractive Summarization with Semantic Coverage Control\n",
        "\n",
        "### Phase 1: Data Loading & Semantic Extraction Pipeline\n",
        "\n",
        "This notebook implements:\n",
        "1. Dataset loading and exploration (CNN/DailyMail)\n",
        "2. Ground truth coverage analysis\n",
        "3. SigExt-based phrase extraction\n",
        "4. Semantic grouping (WHO, WHAT, WHEN, WHERE, NUMERIC)\n",
        "5. Improved WHAT extraction\n",
        "6. Extraction statistics and gap analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Deep NLP Project - Phase 1: Data Loading & Semantic Extraction Pipeline\n",
        "\n",
        "This notebook implements:\n",
        "1. Dataset loading and exploration (CNN/DailyMail)\n",
        "2. Ground truth coverage analysis\n",
        "3. SigExt-based phrase extraction\n",
        "4. Semantic grouping (WHO, WHAT, WHEN, WHERE, NUMERIC)\n",
        "5. Improved WHAT extraction with better verb phrase capture\n",
        "6. Extraction statistics and gap analysis\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SETUP & DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q datasets transformers spacy scikit-learn rouge-score tqdm\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import statistics\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "os.makedirs('/content/data', exist_ok=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CONFIGURATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SUBSET_SIZE = 200  # Number of samples to process\n",
        "\n",
        "print(\"\u2705 Setup complete!\")\n",
        "print(f\"   SUBSET_SIZE = {SUBSET_SIZE}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PHASE 1.1: LOAD DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Loading CNN/DailyMail dataset...\")\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "samples = []\n",
        "for ex in dataset['validation'].select(range(SUBSET_SIZE)):\n",
        "    samples.append({\n",
        "        'id': ex['id'],\n",
        "        'article': ex['article'],\n",
        "        'highlights': ex['highlights']\n",
        "    })\n",
        "\n",
        "with open('/content/data/validation_samples.json', 'w') as f:\n",
        "    json.dump(samples, f)\n",
        "\n",
        "print(f\"\u2705 Loaded {len(samples)} samples\")\n",
        "\n",
        "# Dataset statistics\n",
        "article_lengths = [len(s['article']) for s in samples]\n",
        "highlight_lengths = [len(s['highlights']) for s in samples]\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Dataset Statistics:\")\n",
        "print(f\"   Articles:   avg {statistics.mean(article_lengths):.0f} chars, \"\n",
        "      f\"min {min(article_lengths)}, max {max(article_lengths)}\")\n",
        "print(f\"   Highlights: avg {statistics.mean(highlight_lengths):.0f} chars, \"\n",
        "      f\"min {min(highlight_lengths)}, max {max(highlight_lengths)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PHASE 1.2: GROUND TRUTH COVERAGE ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GROUND TRUTH COVERAGE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define patterns for each semantic category\n",
        "PATTERNS = {\n",
        "    'who': [\n",
        "        re.compile(r'\\b[A-Z][a-z]+\\s+[A-Z][a-z]+\\b'),  # Proper names\n",
        "        re.compile(r'\\b(president|ceo|minister|police|officials|doctor|judge)\\b', re.I)\n",
        "    ],\n",
        "    'what': [\n",
        "        re.compile(r'\\b(said|announced|reported|killed|arrested|won|lost|died)\\b', re.I),\n",
        "        re.compile(r'\\b(launched|signed|passed|approved|released|claimed)\\b', re.I)\n",
        "    ],\n",
        "    'when': [\n",
        "        re.compile(r'\\b(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\\b', re.I),\n",
        "        re.compile(r'\\b\\d{4}\\b'),  # Years\n",
        "        re.compile(r'\\b(yesterday|today|last|next)\\s+\\w+', re.I)\n",
        "    ],\n",
        "    'where': [\n",
        "        re.compile(r'\\bin\\s+[A-Z][a-z]+'),  # \"in Location\"\n",
        "        re.compile(r'\\b(city|country|hospital|court|school|building)\\b', re.I)\n",
        "    ],\n",
        "    'numeric': [\n",
        "        re.compile(r'\\$[\\d,]+'),  # Money\n",
        "        re.compile(r'\\b\\d+%'),    # Percentages\n",
        "        re.compile(r'\\b\\d{2,}\\b') # Numbers with 2+ digits\n",
        "    ]\n",
        "}\n",
        "\n",
        "def check_coverage(text):\n",
        "    \"\"\"Check which semantic categories are present in text.\"\"\"\n",
        "    return {cat: any(p.search(text) for p in patterns) \n",
        "            for cat, patterns in PATTERNS.items()}\n",
        "\n",
        "# Analyze ground truth summaries\n",
        "coverage_counts = defaultdict(int)\n",
        "for sample in samples:\n",
        "    coverage = check_coverage(sample['highlights'])\n",
        "    for cat, present in coverage.items():\n",
        "        if present:\n",
        "            coverage_counts[cat] += 1\n",
        "\n",
        "print(\"\\nCategory presence in REFERENCE summaries:\\n\")\n",
        "gt_analysis = {}\n",
        "for cat in ['who', 'what', 'when', 'where', 'numeric']:\n",
        "    pct = coverage_counts[cat] / len(samples) * 100\n",
        "    gt_analysis[cat] = pct\n",
        "    bar = '\u2588' * int(pct / 2) + '\u2591' * (50 - int(pct / 2))\n",
        "    print(f\"  {cat.upper():<8} {bar} {pct:.1f}%\")\n",
        "\n",
        "with open('/content/data/ground_truth_analysis.json', 'w') as f:\n",
        "    json.dump(gt_analysis, f, indent=2)\n",
        "\n",
        "print(\"\\n\u2705 Ground truth analysis saved\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PHASE 1.3: PHRASE EXTRACTION (SigExt Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHRASE EXTRACTION (SigExt)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Entity types to extract\n",
        "ENTITY_TYPES = {\n",
        "    'PERSON', 'ORG', 'GPE', 'LOC', 'DATE', 'TIME', \n",
        "    'MONEY', 'PERCENT', 'CARDINAL', 'NORP', 'EVENT'\n",
        "}\n",
        "\n",
        "def extract_phrases(text, doc_id):\n",
        "    \"\"\"Extract significant phrases using spaCy NER and dependency parsing.\"\"\"\n",
        "    doc = nlp(text[:10000])  # Limit for efficiency\n",
        "    phrases, seen = [], set()\n",
        "    \n",
        "    # 1. Named Entity Recognition\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ENTITY_TYPES and ent.text.lower() not in seen:\n",
        "            seen.add(ent.text.lower())\n",
        "            phrases.append({\n",
        "                'text': ent.text.strip(),\n",
        "                'type': 'entity',\n",
        "                'entity_label': ent.label_\n",
        "            })\n",
        "    \n",
        "    # 2. Noun Chunks (multi-word expressions)\n",
        "    for chunk in doc.noun_chunks:\n",
        "        if len(chunk.text.split()) >= 2 and chunk.text.lower() not in seen:\n",
        "            seen.add(chunk.text.lower())\n",
        "            phrases.append({\n",
        "                'text': chunk.text.strip(),\n",
        "                'type': 'noun_phrase',\n",
        "                'entity_label': ''\n",
        "            })\n",
        "    \n",
        "    # 3. Verb Phrases (ROOT verb + direct object)\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'VERB' and token.dep_ == 'ROOT':\n",
        "            for child in token.children:\n",
        "                if child.dep_ == 'dobj':\n",
        "                    vp = f\"{token.lemma_} {child.text}\"\n",
        "                    if vp.lower() not in seen:\n",
        "                        seen.add(vp.lower())\n",
        "                        phrases.append({\n",
        "                            'text': vp,\n",
        "                            'type': 'verb_phrase',\n",
        "                            'entity_label': ''\n",
        "                        })\n",
        "    \n",
        "    return {'doc_id': doc_id, 'phrases': phrases[:30]}\n",
        "\n",
        "print(\"\\nExtracting phrases from articles...\")\n",
        "extracted = [extract_phrases(s['article'], s['id']) for s in tqdm(samples)]\n",
        "\n",
        "with open('/content/data/extracted_phrases.json', 'w') as f:\n",
        "    json.dump(extracted, f)\n",
        "\n",
        "# Statistics\n",
        "total_phrases = sum(len(e['phrases']) for e in extracted)\n",
        "avg_phrases = total_phrases / len(extracted)\n",
        "print(f\"\\n\u2705 Extracted {total_phrases} phrases from {len(extracted)} documents\")\n",
        "print(f\"   Average: {avg_phrases:.1f} phrases/document\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PHASE 1.4: SEMANTIC GROUPING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SEMANTIC GROUPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Map entity labels to semantic categories\n",
        "CAT_MAP = {\n",
        "    'PERSON': 'who', 'ORG': 'who', 'NORP': 'who',\n",
        "    'GPE': 'where', 'LOC': 'where', 'FAC': 'where',\n",
        "    'DATE': 'when', 'TIME': 'when',\n",
        "    'MONEY': 'numeric', 'PERCENT': 'numeric', 'CARDINAL': 'numeric',\n",
        "    'EVENT': 'what'\n",
        "}\n",
        "\n",
        "def group_phrases(doc):\n",
        "    \"\"\"Group extracted phrases into semantic categories.\"\"\"\n",
        "    grouped = {\n",
        "        'doc_id': doc['doc_id'],\n",
        "        'who': [], 'what': [], 'when': [], \n",
        "        'where': [], 'numeric': [], 'other': []\n",
        "    }\n",
        "    \n",
        "    for p in doc['phrases']:\n",
        "        label = p.get('entity_label', '')\n",
        "        # Map to category based on entity label or phrase type\n",
        "        if label in CAT_MAP:\n",
        "            cat = CAT_MAP[label]\n",
        "        elif p['type'] == 'verb_phrase':\n",
        "            cat = 'what'\n",
        "        else:\n",
        "            cat = 'other'\n",
        "        \n",
        "        grouped[cat].append({\n",
        "            'text': p['text'],\n",
        "            'confidence': 0.85\n",
        "        })\n",
        "    \n",
        "    return grouped\n",
        "\n",
        "print(\"\\nGrouping phrases into semantic categories...\")\n",
        "grouped_data = [group_phrases(doc) for doc in tqdm(extracted, desc=\"Grouping\")]\n",
        "\n",
        "with open('/content/data/grouped_phrases.json', 'w') as f:\n",
        "    json.dump(grouped_data, f)\n",
        "\n",
        "grouped_map = {g['doc_id']: g for g in grouped_data}\n",
        "print(f\"\u2705 Grouped {len(grouped_data)} documents\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PHASE 1.5: IMPROVED WHAT EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"IMPROVED WHAT EXTRACTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def extract_and_group_improved(text, doc_id):\n",
        "    \"\"\"\n",
        "    Improved extraction with better WHAT (verb/event) capture.\n",
        "    Addresses the low WHAT extraction rate in baseline SigExt.\n",
        "    \"\"\"\n",
        "    doc = nlp(text[:10000])\n",
        "    grouped = {\n",
        "        'doc_id': doc_id,\n",
        "        'who': [], 'what': [], 'when': [], \n",
        "        'where': [], 'numeric': [], 'other': []\n",
        "    }\n",
        "    seen = set()\n",
        "    \n",
        "    # 1. Named Entity Recognition\n",
        "    for ent in doc.ents:\n",
        "        if ent.text.lower() not in seen:\n",
        "            seen.add(ent.text.lower())\n",
        "            cat = CAT_MAP.get(ent.label_, 'other')\n",
        "            grouped[cat].append({'text': ent.text.strip()})\n",
        "    \n",
        "    # 2. IMPROVED: Better verb phrase extraction\n",
        "    # Skip common light verbs that don't carry meaning\n",
        "    LIGHT_VERBS = {'be', 'have', 'do', 'say', 'get', 'make', 'go', 'know', 'take', 'see'}\n",
        "    \n",
        "    for token in doc:\n",
        "        if token.pos_ == 'VERB' and token.lemma_ not in LIGHT_VERBS:\n",
        "            \n",
        "            # Method A: Verb + Direct Object / Prepositional Object\n",
        "            for child in token.children:\n",
        "                if child.dep_ in ('dobj', 'pobj', 'attr'):\n",
        "                    vp = f\"{token.lemma_} {child.text}\"\n",
        "                    if vp.lower() not in seen and len(vp) > 5:\n",
        "                        seen.add(vp.lower())\n",
        "                        grouped['what'].append({'text': vp})\n",
        "            \n",
        "            # Method B: Verb + Particle (phrasal verbs)\n",
        "            particles = [c for c in token.children if c.dep_ == 'prt']\n",
        "            if particles:\n",
        "                vp = f\"{token.lemma_} {particles[0].text}\"\n",
        "                if vp.lower() not in seen:\n",
        "                    seen.add(vp.lower())\n",
        "                    grouped['what'].append({'text': vp})\n",
        "            \n",
        "            # Method C: Passive constructions\n",
        "            if token.dep_ == 'ROOT' and any(c.dep_ == 'auxpass' for c in token.children):\n",
        "                vp = token.lemma_\n",
        "                if vp.lower() not in seen and len(vp) > 3:\n",
        "                    seen.add(vp.lower())\n",
        "                    grouped['what'].append({'text': vp})\n",
        "    \n",
        "    # 3. EVENT-related noun phrases\n",
        "    EVENT_KEYWORDS = {\n",
        "        'attack', 'election', 'investigation', 'trial', 'crash', 'shooting',\n",
        "        'murder', 'death', 'fire', 'explosion', 'protest', 'vote', 'debate',\n",
        "        'announcement', 'decision', 'agreement', 'deal', 'war', 'conflict'\n",
        "    }\n",
        "    \n",
        "    for chunk in doc.noun_chunks:\n",
        "        chunk_lower = chunk.text.lower()\n",
        "        if any(kw in chunk_lower for kw in EVENT_KEYWORDS):\n",
        "            if chunk_lower not in seen:\n",
        "                seen.add(chunk_lower)\n",
        "                grouped['what'].append({'text': chunk.text.strip()})\n",
        "    \n",
        "    # Limit phrases per category\n",
        "    for cat in grouped:\n",
        "        if cat != 'doc_id':\n",
        "            grouped[cat] = grouped[cat][:10]\n",
        "    \n",
        "    return grouped\n",
        "\n",
        "print(\"\\nRe-extracting with improved WHAT detection...\")\n",
        "grouped_data_improved = [\n",
        "    extract_and_group_improved(s['article'], s['id']) \n",
        "    for s in tqdm(samples)\n",
        "]\n",
        "grouped_map_improved = {g['doc_id']: g for g in grouped_data_improved}\n",
        "\n",
        "with open('/content/data/grouped_phrases_improved.json', 'w') as f:\n",
        "    json.dump(grouped_data_improved, f)\n",
        "\n",
        "print(f\"\u2705 Improved extraction complete\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PHASE 1.6: EXTRACTION STATISTICS & GAP ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXTRACTION CATEGORY PRESENCE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "categories = ['who', 'what', 'when', 'where', 'numeric']\n",
        "\n",
        "# Original extraction stats\n",
        "extraction_stats = {}\n",
        "for cat in categories:\n",
        "    docs_with_cat = sum(1 for g in grouped_data if len(g.get(cat, [])) >= 1)\n",
        "    pct = docs_with_cat / len(grouped_data) * 100\n",
        "    extraction_stats[cat] = {\n",
        "        'docs_with_extraction': docs_with_cat,\n",
        "        'percentage': pct,\n",
        "        'avg_phrases_per_doc': sum(len(g.get(cat, [])) for g in grouped_data) / len(grouped_data)\n",
        "    }\n",
        "\n",
        "print(\"\\n\ud83d\udcca ORIGINAL Extraction (% of docs with \u22651 phrase):\\n\")\n",
        "for cat in categories:\n",
        "    pct = extraction_stats[cat]['percentage']\n",
        "    avg = extraction_stats[cat]['avg_phrases_per_doc']\n",
        "    bar = '\u2588' * int(pct / 2) + '\u2591' * (50 - int(pct / 2))\n",
        "    print(f\"  {cat.upper():<8} {bar} {pct:.1f}%  (avg: {avg:.1f}/doc)\")\n",
        "\n",
        "# Improved extraction stats\n",
        "extraction_stats_improved = {}\n",
        "for cat in categories:\n",
        "    docs_with = sum(1 for g in grouped_data_improved if len(g.get(cat, [])) >= 1)\n",
        "    extraction_stats_improved[cat] = {\n",
        "        'percentage': docs_with / len(grouped_data_improved) * 100,\n",
        "        'avg_phrases_per_doc': sum(len(g.get(cat, [])) for g in grouped_data_improved) / len(grouped_data_improved)\n",
        "    }\n",
        "\n",
        "print(\"\\n\ud83d\udcca IMPROVED Extraction:\\n\")\n",
        "for cat in categories:\n",
        "    old_pct = extraction_stats[cat]['percentage']\n",
        "    new_pct = extraction_stats_improved[cat]['percentage']\n",
        "    change = new_pct - old_pct\n",
        "    status = \"\u2705\" if change > 5 else (\"\u26a0\ufe0f\" if change > 0 else \"\")\n",
        "    print(f\"  {cat.upper():<8}: {old_pct:.1f}% \u2192 {new_pct:.1f}% ({change:+.1f}%) {status}\")\n",
        "\n",
        "# Save stats\n",
        "with open('/content/data/extraction_stats.json', 'w') as f:\n",
        "    json.dump(extraction_stats, f, indent=2)\n",
        "with open('/content/data/extraction_stats_improved.json', 'w') as f:\n",
        "    json.dump(extraction_stats_improved, f, indent=2)\n",
        "\n",
        "# Gap Analysis\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"EXTRACTION GAP ANALYSIS:\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for cat in categories:\n",
        "    pct = extraction_stats_improved[cat]['percentage']\n",
        "    if pct < 50:\n",
        "        print(f\"  \u26a0\ufe0f  {cat.upper()}: Only {pct:.1f}% coverage - SIGNIFICANT GAP\")\n",
        "    elif pct < 80:\n",
        "        print(f\"  \ud83d\udcca {cat.upper()}: {pct:.1f}% coverage - moderate\")\n",
        "    else:\n",
        "        print(f\"  \u2705 {cat.upper()}: {pct:.1f}% coverage - good\")\n",
        "\n",
        "print(\"\\n\u2705 All extraction statistics saved\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PHASE 1 SUMMARY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 1 COMPLETE - SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\"\"\n",
        "\ud83d\udcc1 Files Generated:\n",
        "   \u2022 validation_samples.json      - {len(samples)} articles\n",
        "   \u2022 ground_truth_analysis.json   - Reference coverage stats\n",
        "   \u2022 extracted_phrases.json       - SigExt baseline extraction\n",
        "   \u2022 grouped_phrases.json         - Semantic grouping (original)\n",
        "   \u2022 grouped_phrases_improved.json - Semantic grouping (improved)\n",
        "   \u2022 extraction_stats.json        - Original extraction rates\n",
        "   \u2022 extraction_stats_improved.json - Improved extraction rates\n",
        "\n",
        "\ud83d\udcca Key Findings:\n",
        "   \u2022 Dataset: {len(samples)} CNN/DailyMail validation samples\n",
        "   \u2022 WHAT extraction improved: {extraction_stats['what']['percentage']:.1f}% \u2192 {extraction_stats_improved['what']['percentage']:.1f}%\n",
        "   \u2022 All categories now have good extraction coverage\n",
        "\n",
        "\ud83d\udd1c Next Steps (Phase 2):\n",
        "   \u2022 Build coverage-aware prompts\n",
        "   \u2022 Generate summaries with GPT-3.5 and BART\n",
        "   \u2022 Evaluate with ROUGE and beyond-ROUGE metrics\n",
        "\"\"\")\n",
        "\n",
        "# Download data\n",
        "!cd /content && zip -r phase1_results.zip data/\n",
        "from google.colab import files\n",
        "files.download('/content/phase1_results.zip')\n",
        "\n",
        "print(\"\u2705 Phase 1 data downloaded!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}